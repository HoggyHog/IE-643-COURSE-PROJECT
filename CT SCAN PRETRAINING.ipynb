{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":861496,"sourceType":"datasetVersion","datasetId":457093},{"sourceId":7056571,"sourceType":"datasetVersion","datasetId":4061834}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightly -q","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:12:15.319122Z","iopub.execute_input":"2023-11-27T11:12:15.319443Z","iopub.status.idle":"2023-11-27T11:12:32.481638Z","shell.execute_reply.started":"2023-11-27T11:12:15.319401Z","shell.execute_reply":"2023-11-27T11:12:32.480592Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#dependencies for file management and data visualisation\nimport os\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nibabel as nib\nimport re\nfrom tqdm import tqdm\nimport pandas as pd\nimport gc\nimport psutil\nfrom matplotlib.animation import FuncAnimation\nimport seaborn as sns\nfrom IPython.display import HTML\nimport cv2\n\n#pretraining dependencies\nfrom lightly.data import LightlyDataset\n\n#pretraining of SIMCLR MODEL\nfrom lightly.transforms.simclr_transform import SimCLRTransform\nfrom lightly.loss import NTXentLoss\nfrom lightly.models.modules.heads import SimCLRProjectionHead\n\n#pretraining of BarlowTwins MODEL\nfrom lightly.loss import BarlowTwinsLoss\nfrom lightly.models.modules import BarlowTwinsProjectionHead\nfrom lightly.transforms.byol_transform import (\n    BYOLTransform,\n    BYOLView1Transform,\n    BYOLView2Transform,\n)\n\n#pretraining on SWaV MODEL\nfrom lightly.loss import SwaVLoss\nfrom lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\nfrom lightly.transforms.swav_transform import SwaVTransform\n\n\n#fine-tuning the models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader , random_split\nimport pytorch_lightning as pl\nimport torchvision\nfrom torchvision import transforms\nfrom pytorch_lightning import seed_everything\n\n#evaluating the models\nfrom sklearn.metrics import log_loss\n\nseed_value = 50\n\ntorch.manual_seed(seed_value)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed_value)\n\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:12:32.483431Z","iopub.execute_input":"2023-11-27T11:12:32.483779Z","iopub.status.idle":"2023-11-27T11:12:42.433580Z","shell.execute_reply.started":"2023-11-27T11:12:32.483749Z","shell.execute_reply":"2023-11-27T11:12:42.432663Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}]},{"cell_type":"markdown","source":"GETTING ALL THE MRI IMAGES INTO ONE SINGLE FOLDER","metadata":{}},{"cell_type":"code","source":"final_loc='/kaggle/data'\nc=0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T11:12:42.434646Z","iopub.execute_input":"2023-11-27T11:12:42.435082Z","iopub.status.idle":"2023-11-27T11:12:42.439187Z","shell.execute_reply.started":"2023-11-27T11:12:42.435058Z","shell.execute_reply":"2023-11-27T11:12:42.438264Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_path='/kaggle/input/ct-scans-2/CT_img_1'\n\nos.makedirs(final_loc,exist_ok=True)\nfor file in os.listdir(data_path):\n    c+=1\n    \n    file_path=os.path.join(data_path,file)\n    ext=file[-4:]\n    if(ext=='jpeg'):\n        ext='.jpeg'\n    new_filename=str(c)+ext\n    final_path=os.path.join(final_loc,new_filename)\n    \n    shutil.copy(file_path, final_path)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:12:42.441673Z","iopub.execute_input":"2023-11-27T11:12:42.441932Z","iopub.status.idle":"2023-11-27T11:13:02.447560Z","shell.execute_reply.started":"2023-11-27T11:12:42.441909Z","shell.execute_reply":"2023-11-27T11:13:02.446764Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"counter=0\nfor i in os.listdir(final_loc):\n    counter+=1\nprint(c,counter)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:02.448640Z","iopub.execute_input":"2023-11-27T11:13:02.448921Z","iopub.status.idle":"2023-11-27T11:13:02.456781Z","shell.execute_reply.started":"2023-11-27T11:13:02.448896Z","shell.execute_reply":"2023-11-27T11:13:02.455789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2987 2987\n","output_type":"stream"}]},{"cell_type":"markdown","source":"MAKING THE SIMCLR MODEL NOW","metadata":{}},{"cell_type":"code","source":"input_size=32\nSC_Imagenet_transform = SimCLRTransform(input_size=input_size)\n\n# Create a dataset from your image folder.\nSC_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SC_Imagenet_transform,\n)\n\n# Build a PyTorch dataloader.\nSC_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SC_Imagenet_dataset,                # Pass the dataset to the dataloader.\n    batch_size=16,         # A large batch size helps with learning.\n    shuffle=True,           # Shuffling is important!\n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:02.457809Z","iopub.execute_input":"2023-11-27T11:13:02.458068Z","iopub.status.idle":"2023-11-27T11:13:02.485023Z","shell.execute_reply.started":"2023-11-27T11:13:02.458045Z","shell.execute_reply":"2023-11-27T11:13:02.484204Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_epochs=25\n\nclass SimCLR(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n        \n        self.projection_head = SimCLRProjectionHead(512,512,2048)\n        #1st parameter -> input size (from the last layer of the resnet backbone)\n        #2nd parameter -> hidden size\n        #3rd parameter -> output size\n\n        self.criterion = NTXentLoss()\n\n    def forward(self, x):\n        h = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(h)\n        return z\n\n    def training_step(self, batch, batch_idx):\n        (x0, x1), _, _ = batch\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(\n            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n        )\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n        return [optim],[scheduler]\n    \ntorch.cuda.empty_cache()\nSimCLR_ImagenetPT_model = SimCLR()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SimCLR_ImagenetPT_model, SC_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:13:02.486083Z","iopub.execute_input":"2023-11-27T11:13:02.486337Z","iopub.status.idle":"2023-11-27T11:17:00.837459Z","shell.execute_reply.started":"2023-11-27T11:13:02.486314Z","shell.execute_reply":"2023-11-27T11:17:00.836566Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df59799a4fb463eb8633fbec8340293"}},"metadata":{}}]},{"cell_type":"markdown","source":"MAKING THE BARLOWTWINS MODEL NOW","metadata":{}},{"cell_type":"code","source":"BT_Imagenet_transform = BYOLTransform(\n    view_1_transform=BYOLView1Transform(input_size=32, gaussian_blur=0.0),\n    view_2_transform=BYOLView2Transform(input_size=32, gaussian_blur=0.0),\n)\n\nBT_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=BT_Imagenet_transform,\n)\n\nBT_Imagenet_dataloader = torch.utils.data.DataLoader(\n    BT_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:17:00.839066Z","iopub.execute_input":"2023-11-27T11:17:00.839392Z","iopub.status.idle":"2023-11-27T11:17:00.861675Z","shell.execute_reply.started":"2023-11-27T11:17:00.839361Z","shell.execute_reply":"2023-11-27T11:17:00.860981Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"max_epochs=25\n\nclass BarlowTwins(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n        self.criterion = BarlowTwinsLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(x)\n        return z\n\n    def training_step(self, batch, batch_index):\n        (x0, x1) = batch[0]\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n        return optim\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:17:00.862711Z","iopub.execute_input":"2023-11-27T11:17:00.862959Z","iopub.status.idle":"2023-11-27T11:17:00.871268Z","shell.execute_reply.started":"2023-11-27T11:17:00.862930Z","shell.execute_reply":"2023-11-27T11:17:00.870452Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n#resnet = torchvision.models.resnet18()\nBT_ImagenetPT_model = BarlowTwins()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(BT_ImagenetPT_model, BT_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:17:00.874910Z","iopub.execute_input":"2023-11-27T11:17:00.875184Z","iopub.status.idle":"2023-11-27T11:20:30.108874Z","shell.execute_reply.started":"2023-11-27T11:17:00.875160Z","shell.execute_reply":"2023-11-27T11:20:30.107985Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6033553b7048e6aaf1b3f58bec1407"}},"metadata":{}}]},{"cell_type":"markdown","source":"PRETRAINING OVER SWAV MODEL NOW","metadata":{}},{"cell_type":"code","source":"SW_Imagenet_transform = SwaVTransform()\n# we ignore object detection annotations by setting target_transform to return 0\nSW_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SW_Imagenet_transform,\n)\n# or create a dataset from a folder containing images or videos:\n# dataset = LightlyDataset(\"path/to/folder\")\n\nSW_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SW_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:20:30.110373Z","iopub.execute_input":"2023-11-27T11:20:30.110693Z","iopub.status.idle":"2023-11-27T11:20:30.134409Z","shell.execute_reply.started":"2023-11-27T11:20:30.110663Z","shell.execute_reply":"2023-11-27T11:20:30.133606Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_epochs=15\nclass SwaV(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = SwaVProjectionHead(512, 512, 128)\n        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n        self.criterion = SwaVLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        x = self.projection_head(x)\n        x = nn.functional.normalize(x, dim=1, p=2)\n        p = self.prototypes(x)\n        return p\n\n    def training_step(self, batch, batch_idx):\n        self.prototypes.normalize()\n        views = batch[0]\n        multi_crop_features = [self.forward(view.to(self.device)) for view in views]\n        high_resolution = multi_crop_features[:2]\n        low_resolution = multi_crop_features[2:]\n        loss = self.criterion(high_resolution, low_resolution)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optim\n\ntorch.cuda.empty_cache()\nSW_ImagenetPT_model = SwaV()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SW_ImagenetPT_model, SW_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:20:30.135603Z","iopub.execute_input":"2023-11-27T11:20:30.135936Z","iopub.status.idle":"2023-11-27T11:30:52.344559Z","shell.execute_reply.started":"2023-11-27T11:20:30.135903Z","shell.execute_reply":"2023-11-27T11:30:52.343415Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf0b01f1b6446e7bb84fb6180e6f33d"}},"metadata":{}}]},{"cell_type":"markdown","source":"NOW THAT THE PRETRAINING IS DONE, WE DEFINE THE TRAIN AND TEST DATALOADER FOR THE FINE-TUNING TASK","metadata":{}},{"cell_type":"code","source":"def one_hot(n):\n    index={0:0,0.5:1,1:2,2:3}\n    #print(index)\n    #print(index[n])\n    y_new=np.zeros(4)\n    y_new[index[n]]=1\n    #print(y_new)\n    return y_new","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:30:52.346859Z","iopub.execute_input":"2023-11-27T11:30:52.347189Z","iopub.status.idle":"2023-11-27T11:30:52.355189Z","shell.execute_reply.started":"2023-11-27T11:30:52.347158Z","shell.execute_reply":"2023-11-27T11:30:52.354282Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"c=0\ndf_new = pd.DataFrame(columns=['IMG', 'CDR'])\ndef make_dataset(path,cdr):\n    \n    global c, df_new\n    \n    for i in os.listdir(path):\n        c+=1\n        image = cv2.imread(os.path.join(path,i))\n        image_np = np.array(image)\n        image_tensor = transforms.ToTensor()(image_np)\n        padded_image = transforms.CenterCrop((224, 224))(transforms.Pad(28)(image_tensor))\n        #print(padded_image.shape)\n        '''plt.imshow(padded_image.permute(1, 2, 0))\n        print('padded_image',padded_image.shape)'''  \n        \n        new_entry = {\n            'IMG': padded_image,\n            'CDR': cdr\n        } \n        df_new = pd.concat([df_new, pd.DataFrame([new_entry])], ignore_index=True)\n        #print(i)\n\n\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented',0.5)\n        \nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/VeryMildDemented',0.5)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:30:52.356721Z","iopub.execute_input":"2023-11-27T11:30:52.357368Z","iopub.status.idle":"2023-11-27T11:31:38.792407Z","shell.execute_reply.started":"2023-11-27T11:30:52.357333Z","shell.execute_reply":"2023-11-27T11:31:38.791591Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(c,df_new.shape)\nprint(df_new['IMG'][0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:38.793666Z","iopub.execute_input":"2023-11-27T11:31:38.793956Z","iopub.status.idle":"2023-11-27T11:31:38.803149Z","shell.execute_reply.started":"2023-11-27T11:31:38.793919Z","shell.execute_reply":"2023-11-27T11:31:38.802088Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"6400 (6400, 2)\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"x=np.stack(df_new['IMG'].tolist())\ny=df_new['CDR']\n\ny_one_hot=[]\nfor i in y:\n    y_one_hot.append(one_hot(i)) ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:38.804340Z","iopub.execute_input":"2023-11-27T11:31:38.804644Z","iopub.status.idle":"2023-11-27T11:31:40.032637Z","shell.execute_reply.started":"2023-11-27T11:31:38.804620Z","shell.execute_reply":"2023-11-27T11:31:40.031444Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(type(x))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:40.033808Z","iopub.execute_input":"2023-11-27T11:31:40.034092Z","iopub.status.idle":"2023-11-27T11:31:40.039202Z","shell.execute_reply.started":"2023-11-27T11:31:40.034068Z","shell.execute_reply":"2023-11-27T11:31:40.038313Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"x=torch.tensor(x)\ny=torch.Tensor(y_one_hot)\n\ndataset=TensorDataset(x,y)\n\ntrain_size=int(0.6*len(x))\nval_size=int(0.2*len(x))\ntemp_size=2*val_size\n\n#print(train_size,val_size,temp_size)\n\ntrain_dataset, temp_dataset = random_split(dataset, [train_size, temp_size])\nval_dataset,test_dataset=random_split(temp_dataset, [val_size,val_size])\n\n# Define batch size\n#batch_size = 8\n\n# Create a DataLoader for shuffling and batching\n'''train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\ntest_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:40.040386Z","iopub.execute_input":"2023-11-27T11:31:40.040755Z","iopub.status.idle":"2023-11-27T11:31:41.351010Z","shell.execute_reply.started":"2023-11-27T11:31:40.040727Z","shell.execute_reply":"2023-11-27T11:31:41.350098Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2762140771.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  y=torch.Tensor(y_one_hot)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\\ntest_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)'"},"metadata":{}}]},{"cell_type":"code","source":"def train_and_val(backbone):\n    \n    #training the models first on train_dataloader\n    \n    models=[]\n    for i in range(2):\n        models.append(ResnetSingleChannel(backbone,4))\n        \n   \n    \n    batch_sizes=[64,32]\n    \n    for model_no in range(2):\n        print(f'TRAINING MODEL {model_no+1}')\n        print('--------------------------------------------------------------')\n        fine_tune_opt = optim.Adam(models[model_no].parameters(), lr=0.001, weight_decay=0.0001)\n        ft_loss_fn= nn.CrossEntropyLoss()\n        \n        batch_size=batch_sizes[model_no]\n        \n        train_dataloader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n        \n\n        num_epochs = 8\n        total_steps=train_size//batch_size\n\n        for epoch in range(num_epochs):\n            i=0\n\n            for x_batch,y_batch in train_dataloader:\n                i+=1\n                outputs=models[model_no](torch.Tensor(x_batch))\n                \n                '''print('out',outputs.shape)\n                print('y',y_batch.shape)\n                print('out',outputs)\n                print('y',y_batch)'''\n                \n                loss = ft_loss_fn(outputs, y_batch)\n                fine_tune_opt.zero_grad()\n                loss.backward()\n                fine_tune_opt.step()\n\n                if (i+1) % 20 == 0:\n                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n    \n    # performing validation on validation data_loader\n    best_model=-1\n    best_loss=np.inf\n    for model_no in range(2):\n        \n        loss_sum=0\n        \n        for x_batch,y_batch in val_dataloader:\n            outputs=models[model_no](torch.Tensor(x_batch))\n            loss = ft_loss_fn(outputs, y_batch)\n            loss_sum+=loss.item()\n            \n        if(loss_sum<best_loss):\n            best_model=model_no\n            best_loss=loss_sum\n    \n    return models[best_model]\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:41.352205Z","iopub.execute_input":"2023-11-27T11:31:41.352501Z","iopub.status.idle":"2023-11-27T11:31:41.363892Z","shell.execute_reply.started":"2023-11-27T11:31:41.352476Z","shell.execute_reply":"2023-11-27T11:31:41.362894Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    return F.softmax(x, dim=-1)\n\ndef metrics(model):\n    \n    test_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)\n    \n    y_pred_M=[]\n    y_true_M=[]\n    c=0\n    logloss_sum=0\n    \n    for x_batch,y_batch in test_dataloader:\n        \n        c+=1\n        y_pred=model(x_batch)\n\n        '''y_pred_M.append(np.argmax(y_pred.detach().numpy()))\n        y_true_M.append(np.argmax(y_batch.detach().numpy()))'''\n\n        softmax_probs = softmax(y_pred)\n\n        '''print(y_pred)\n        print(softmax_probs)\n        print(y_batch)'''\n\n        logloss = log_loss(y_batch.detach().numpy(), softmax_probs.detach().numpy())\n\n        logloss_sum+=logloss\n        #print(logloss) \n\n    return(logloss_sum/c)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:41.365056Z","iopub.execute_input":"2023-11-27T11:31:41.365364Z","iopub.status.idle":"2023-11-27T11:31:41.382041Z","shell.execute_reply.started":"2023-11-27T11:31:41.365339Z","shell.execute_reply":"2023-11-27T11:31:41.381289Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#MAKING A NEW NETWORK WHICH USES THE BACKBONE OF THE PRETRAINED MODEL, AND HAS AN INPUT LAYER TO TAKE IN A SINGLE CHANNEL IMAGE, AND THEN PASS INTO \n#BACKBONE, AND FINALLY OUTPUT LAYER WHICH PERFORMS REGRESSION\n\nclass ResnetSingleChannel(nn.Module):\n    def __init__(self,backbone,channels):\n        super(ResnetSingleChannel,self).__init__()\n        self.input=nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1, bias=False)\n        self.backbone=backbone\n        self.output=nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.Linear(in_features=256, out_features=channels)\n        )\n        \n    def forward(self,x):\n        x=self.input(x)\n        x=self.backbone(x)\n        x=self.output(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:41.383217Z","iopub.execute_input":"2023-11-27T11:31:41.383576Z","iopub.status.idle":"2023-11-27T11:31:41.396047Z","shell.execute_reply.started":"2023-11-27T11:31:41.383527Z","shell.execute_reply":"2023-11-27T11:31:41.395245Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**KEEPING THE BACKBONE UNFROZEN**","metadata":{}},{"cell_type":"code","source":"for layer in SimCLR_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSC_MODEL=train_and_val(SimCLR_ImagenetPT_model.backbone)\nSC_M_2=metrics(SC_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T11:31:41.397157Z","iopub.execute_input":"2023-11-27T11:31:41.397437Z","iopub.status.idle":"2023-11-27T12:51:49.601443Z","shell.execute_reply.started":"2023-11-27T11:31:41.397406Z","shell.execute_reply":"2023-11-27T12:51:49.600521Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 0.9522\nEpoch [1/8], Step [40/60], Loss: 0.9032\nEpoch [1/8], Step [60/60], Loss: 0.8346\nEpoch [2/8], Step [20/60], Loss: 0.7374\nEpoch [2/8], Step [40/60], Loss: 0.6542\nEpoch [2/8], Step [60/60], Loss: 0.7728\nEpoch [3/8], Step [20/60], Loss: 0.7502\nEpoch [3/8], Step [40/60], Loss: 0.7464\nEpoch [3/8], Step [60/60], Loss: 0.9391\nEpoch [4/8], Step [20/60], Loss: 0.4308\nEpoch [4/8], Step [40/60], Loss: 0.5457\nEpoch [4/8], Step [60/60], Loss: 0.3963\nEpoch [5/8], Step [20/60], Loss: 0.4343\nEpoch [5/8], Step [40/60], Loss: 0.2523\nEpoch [5/8], Step [60/60], Loss: 0.2744\nEpoch [6/8], Step [20/60], Loss: 0.2748\nEpoch [6/8], Step [40/60], Loss: 0.3079\nEpoch [6/8], Step [60/60], Loss: 0.2419\nEpoch [7/8], Step [20/60], Loss: 0.1854\nEpoch [7/8], Step [40/60], Loss: 0.2664\nEpoch [7/8], Step [60/60], Loss: 0.3010\nEpoch [8/8], Step [20/60], Loss: 0.1360\nEpoch [8/8], Step [40/60], Loss: 0.0294\nEpoch [8/8], Step [60/60], Loss: 0.1363\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 0.9200\nEpoch [1/8], Step [40/120], Loss: 0.8378\nEpoch [1/8], Step [60/120], Loss: 1.0021\nEpoch [1/8], Step [80/120], Loss: 0.8559\nEpoch [1/8], Step [100/120], Loss: 0.8873\nEpoch [1/8], Step [120/120], Loss: 0.7295\nEpoch [2/8], Step [20/120], Loss: 0.6278\nEpoch [2/8], Step [40/120], Loss: 0.6819\nEpoch [2/8], Step [60/120], Loss: 0.6614\nEpoch [2/8], Step [80/120], Loss: 0.5833\nEpoch [2/8], Step [100/120], Loss: 0.6934\nEpoch [2/8], Step [120/120], Loss: 0.6225\nEpoch [3/8], Step [20/120], Loss: 0.5027\nEpoch [3/8], Step [40/120], Loss: 0.5445\nEpoch [3/8], Step [60/120], Loss: 0.3430\nEpoch [3/8], Step [80/120], Loss: 0.3230\nEpoch [3/8], Step [100/120], Loss: 0.3977\nEpoch [3/8], Step [120/120], Loss: 0.3702\nEpoch [4/8], Step [20/120], Loss: 0.2733\nEpoch [4/8], Step [40/120], Loss: 0.3606\nEpoch [4/8], Step [60/120], Loss: 0.2052\nEpoch [4/8], Step [80/120], Loss: 0.3014\nEpoch [4/8], Step [100/120], Loss: 0.3122\nEpoch [4/8], Step [120/120], Loss: 0.1948\nEpoch [5/8], Step [20/120], Loss: 0.0760\nEpoch [5/8], Step [40/120], Loss: 0.0970\nEpoch [5/8], Step [60/120], Loss: 0.0602\nEpoch [5/8], Step [80/120], Loss: 0.2445\nEpoch [5/8], Step [100/120], Loss: 0.3074\nEpoch [5/8], Step [120/120], Loss: 0.2008\nEpoch [6/8], Step [20/120], Loss: 0.1416\nEpoch [6/8], Step [40/120], Loss: 0.3211\nEpoch [6/8], Step [60/120], Loss: 0.0636\nEpoch [6/8], Step [80/120], Loss: 0.0283\nEpoch [6/8], Step [100/120], Loss: 0.0265\nEpoch [6/8], Step [120/120], Loss: 0.0865\nEpoch [7/8], Step [20/120], Loss: 0.0259\nEpoch [7/8], Step [40/120], Loss: 0.0247\nEpoch [7/8], Step [60/120], Loss: 0.3813\nEpoch [7/8], Step [80/120], Loss: 0.0894\nEpoch [7/8], Step [100/120], Loss: 0.0228\nEpoch [7/8], Step [120/120], Loss: 0.1599\nEpoch [8/8], Step [20/120], Loss: 0.0133\nEpoch [8/8], Step [40/120], Loss: 0.0029\nEpoch [8/8], Step [60/120], Loss: 0.1391\nEpoch [8/8], Step [80/120], Loss: 0.2263\nEpoch [8/8], Step [100/120], Loss: 0.3049\nEpoch [8/8], Step [120/120], Loss: 0.0392\n","output_type":"stream"}]},{"cell_type":"code","source":"for layer in BT_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nBT_MODEL=train_and_val(BT_ImagenetPT_model.backbone)\nBT_M_2=metrics(BT_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T12:51:49.602872Z","iopub.execute_input":"2023-11-27T12:51:49.603484Z","iopub.status.idle":"2023-11-27T14:28:00.731435Z","shell.execute_reply.started":"2023-11-27T12:51:49.603446Z","shell.execute_reply":"2023-11-27T14:28:00.730233Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 1.1895\nEpoch [1/8], Step [40/60], Loss: 1.1621\nEpoch [1/8], Step [60/60], Loss: 1.1004\nEpoch [2/8], Step [20/60], Loss: 1.0181\nEpoch [2/8], Step [40/60], Loss: 1.4636\nEpoch [2/8], Step [60/60], Loss: 1.2567\nEpoch [3/8], Step [20/60], Loss: 1.1022\nEpoch [3/8], Step [40/60], Loss: 1.0371\nEpoch [3/8], Step [60/60], Loss: 1.0991\nEpoch [4/8], Step [20/60], Loss: 1.0467\nEpoch [4/8], Step [40/60], Loss: 1.1119\nEpoch [4/8], Step [60/60], Loss: 1.1012\nEpoch [5/8], Step [20/60], Loss: 1.1756\nEpoch [5/8], Step [40/60], Loss: 1.1035\nEpoch [5/8], Step [60/60], Loss: 0.9628\nEpoch [6/8], Step [20/60], Loss: 0.9748\nEpoch [6/8], Step [40/60], Loss: 0.9543\nEpoch [6/8], Step [60/60], Loss: 0.9647\nEpoch [7/8], Step [20/60], Loss: 0.8936\nEpoch [7/8], Step [40/60], Loss: 1.0971\nEpoch [7/8], Step [60/60], Loss: 1.1290\nEpoch [8/8], Step [20/60], Loss: 1.0406\nEpoch [8/8], Step [40/60], Loss: 1.0065\nEpoch [8/8], Step [60/60], Loss: 0.8327\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 2.5549\nEpoch [1/8], Step [40/120], Loss: 1.8507\nEpoch [1/8], Step [60/120], Loss: 0.9114\nEpoch [1/8], Step [80/120], Loss: 1.1906\nEpoch [1/8], Step [100/120], Loss: 1.2933\nEpoch [1/8], Step [120/120], Loss: 1.1827\nEpoch [2/8], Step [20/120], Loss: 0.9741\nEpoch [2/8], Step [40/120], Loss: 1.3001\nEpoch [2/8], Step [60/120], Loss: 1.0277\nEpoch [2/8], Step [80/120], Loss: 0.9775\nEpoch [2/8], Step [100/120], Loss: 0.8769\nEpoch [2/8], Step [120/120], Loss: 1.0520\nEpoch [3/8], Step [20/120], Loss: 1.2381\nEpoch [3/8], Step [40/120], Loss: 1.0833\nEpoch [3/8], Step [60/120], Loss: 0.8670\nEpoch [3/8], Step [80/120], Loss: 1.1974\nEpoch [3/8], Step [100/120], Loss: 0.8518\nEpoch [3/8], Step [120/120], Loss: 0.8367\nEpoch [4/8], Step [20/120], Loss: 1.0640\nEpoch [4/8], Step [40/120], Loss: 1.0376\nEpoch [4/8], Step [60/120], Loss: 0.7247\nEpoch [4/8], Step [80/120], Loss: 1.0072\nEpoch [4/8], Step [100/120], Loss: 1.0877\nEpoch [4/8], Step [120/120], Loss: 0.9563\nEpoch [5/8], Step [20/120], Loss: 0.9587\nEpoch [5/8], Step [40/120], Loss: 0.9407\nEpoch [5/8], Step [60/120], Loss: 0.8484\nEpoch [5/8], Step [80/120], Loss: 1.4497\nEpoch [5/8], Step [100/120], Loss: 0.8080\nEpoch [5/8], Step [120/120], Loss: 0.8133\nEpoch [6/8], Step [20/120], Loss: 0.9107\nEpoch [6/8], Step [40/120], Loss: 0.9531\nEpoch [6/8], Step [60/120], Loss: 0.9435\nEpoch [6/8], Step [80/120], Loss: 0.9144\nEpoch [6/8], Step [100/120], Loss: 0.9149\nEpoch [6/8], Step [120/120], Loss: 0.9411\nEpoch [7/8], Step [20/120], Loss: 0.9982\nEpoch [7/8], Step [40/120], Loss: 0.9601\nEpoch [7/8], Step [60/120], Loss: 0.8926\nEpoch [7/8], Step [80/120], Loss: 0.9390\nEpoch [7/8], Step [100/120], Loss: 0.7611\nEpoch [7/8], Step [120/120], Loss: 0.8185\nEpoch [8/8], Step [20/120], Loss: 0.9671\nEpoch [8/8], Step [40/120], Loss: 0.7511\nEpoch [8/8], Step [60/120], Loss: 1.1103\nEpoch [8/8], Step [80/120], Loss: 0.7639\nEpoch [8/8], Step [100/120], Loss: 0.8293\nEpoch [8/8], Step [120/120], Loss: 0.8838\n","output_type":"stream"}]},{"cell_type":"code","source":"for layer in SW_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSW_MODEL=train_and_val(SW_ImagenetPT_model.backbone)\nSW_M_2=metrics(SW_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T14:28:00.732942Z","iopub.execute_input":"2023-11-27T14:28:00.733641Z","iopub.status.idle":"2023-11-27T15:54:09.602601Z","shell.execute_reply.started":"2023-11-27T14:28:00.733601Z","shell.execute_reply":"2023-11-27T15:54:09.601592Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 1.0218\nEpoch [1/8], Step [40/60], Loss: 0.7916\nEpoch [1/8], Step [60/60], Loss: 0.9214\nEpoch [2/8], Step [20/60], Loss: 0.8782\nEpoch [2/8], Step [40/60], Loss: 1.0232\nEpoch [2/8], Step [60/60], Loss: 0.9126\nEpoch [3/8], Step [20/60], Loss: 0.8705\nEpoch [3/8], Step [40/60], Loss: 0.8530\nEpoch [3/8], Step [60/60], Loss: 0.7327\nEpoch [4/8], Step [20/60], Loss: 0.6741\nEpoch [4/8], Step [40/60], Loss: 0.8561\nEpoch [4/8], Step [60/60], Loss: 0.8215\nEpoch [5/8], Step [20/60], Loss: 0.9003\nEpoch [5/8], Step [40/60], Loss: 0.8323\nEpoch [5/8], Step [60/60], Loss: 0.8250\nEpoch [6/8], Step [20/60], Loss: 0.7093\nEpoch [6/8], Step [40/60], Loss: 0.5818\nEpoch [6/8], Step [60/60], Loss: 0.4924\nEpoch [7/8], Step [20/60], Loss: 0.8866\nEpoch [7/8], Step [40/60], Loss: 0.5955\nEpoch [7/8], Step [60/60], Loss: 0.4672\nEpoch [8/8], Step [20/60], Loss: 0.4573\nEpoch [8/8], Step [40/60], Loss: 0.3895\nEpoch [8/8], Step [60/60], Loss: 0.4807\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 1.0707\nEpoch [1/8], Step [40/120], Loss: 0.9218\nEpoch [1/8], Step [60/120], Loss: 0.7254\nEpoch [1/8], Step [80/120], Loss: 0.9650\nEpoch [1/8], Step [100/120], Loss: 0.8531\nEpoch [1/8], Step [120/120], Loss: 0.7535\nEpoch [2/8], Step [20/120], Loss: 0.7210\nEpoch [2/8], Step [40/120], Loss: 0.8642\nEpoch [2/8], Step [60/120], Loss: 0.9478\nEpoch [2/8], Step [80/120], Loss: 1.1722\nEpoch [2/8], Step [100/120], Loss: 0.7902\nEpoch [2/8], Step [120/120], Loss: 0.9605\nEpoch [3/8], Step [20/120], Loss: 0.9804\nEpoch [3/8], Step [40/120], Loss: 0.8122\nEpoch [3/8], Step [60/120], Loss: 0.8466\nEpoch [3/8], Step [80/120], Loss: 0.8027\nEpoch [3/8], Step [100/120], Loss: 0.7792\nEpoch [3/8], Step [120/120], Loss: 0.8187\nEpoch [4/8], Step [20/120], Loss: 0.8178\nEpoch [4/8], Step [40/120], Loss: 0.8501\nEpoch [4/8], Step [60/120], Loss: 1.0616\nEpoch [4/8], Step [80/120], Loss: 0.7815\nEpoch [4/8], Step [100/120], Loss: 0.5981\nEpoch [4/8], Step [120/120], Loss: 0.6922\nEpoch [5/8], Step [20/120], Loss: 0.8285\nEpoch [5/8], Step [40/120], Loss: 0.8500\nEpoch [5/8], Step [60/120], Loss: 0.8483\nEpoch [5/8], Step [80/120], Loss: 0.8396\nEpoch [5/8], Step [100/120], Loss: 0.7395\nEpoch [5/8], Step [120/120], Loss: 0.6194\nEpoch [6/8], Step [20/120], Loss: 0.7773\nEpoch [6/8], Step [40/120], Loss: 0.7671\nEpoch [6/8], Step [60/120], Loss: 0.6782\nEpoch [6/8], Step [80/120], Loss: 0.6354\nEpoch [6/8], Step [100/120], Loss: 0.4938\nEpoch [6/8], Step [120/120], Loss: 0.6198\nEpoch [7/8], Step [20/120], Loss: 0.5671\nEpoch [7/8], Step [40/120], Loss: 0.6227\nEpoch [7/8], Step [60/120], Loss: 0.4450\nEpoch [7/8], Step [80/120], Loss: 0.5420\nEpoch [7/8], Step [100/120], Loss: 0.6731\nEpoch [7/8], Step [120/120], Loss: 0.5385\nEpoch [8/8], Step [20/120], Loss: 0.3793\nEpoch [8/8], Step [40/120], Loss: 0.6191\nEpoch [8/8], Step [60/120], Loss: 0.3963\nEpoch [8/8], Step [80/120], Loss: 0.6850\nEpoch [8/8], Step [100/120], Loss: 0.4477\nEpoch [8/8], Step [120/120], Loss: 0.4978\n","output_type":"stream"}]},{"cell_type":"code","source":"print('SimCLR',SC_M_2)\nprint('Barlow',BT_M_2)\nprint('SwAV',SW_M_2)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T15:54:09.603929Z","iopub.execute_input":"2023-11-27T15:54:09.604224Z","iopub.status.idle":"2023-11-27T15:54:09.609512Z","shell.execute_reply.started":"2023-11-27T15:54:09.604198Z","shell.execute_reply":"2023-11-27T15:54:09.608586Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"SimCLR 1.2063910560350524\nBarlow 1.0927559791338648\nSwAV 1.2759309196134094\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}