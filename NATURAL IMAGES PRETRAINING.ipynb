{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":993979,"sourceType":"datasetVersion","datasetId":544502},{"sourceId":995044,"sourceType":"datasetVersion","datasetId":545156},{"sourceId":2738526,"sourceType":"datasetVersion","datasetId":1669494},{"sourceId":6677526,"sourceType":"datasetVersion","datasetId":3852514},{"sourceId":861496,"sourceType":"datasetVersion","datasetId":457093}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"INSTALLING THE REQUIRED DEPENDENCIES","metadata":{}},{"cell_type":"code","source":"!pip install lightly -q","metadata":{"execution":{"iopub.status.busy":"2023-11-26T08:43:07.911278Z","iopub.execute_input":"2023-11-26T08:43:07.911661Z","iopub.status.idle":"2023-11-26T08:43:25.725158Z","shell.execute_reply.started":"2023-11-26T08:43:07.911627Z","shell.execute_reply":"2023-11-26T08:43:25.723821Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"IMPORTING THE REQUIRED DEPENDENCIES","metadata":{}},{"cell_type":"code","source":"#dependencies for file management and data visualisation\nimport os\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nibabel as nib\nimport re\nfrom tqdm import tqdm\nimport pandas as pd\nimport gc\nimport psutil\nfrom matplotlib.animation import FuncAnimation\nimport seaborn as sns\nfrom IPython.display import HTML\nimport cv2\n\n#pretraining dependencies\nfrom lightly.data import LightlyDataset\n\n#pretraining of SIMCLR MODEL\nfrom lightly.transforms.simclr_transform import SimCLRTransform\nfrom lightly.loss import NTXentLoss\nfrom lightly.models.modules.heads import SimCLRProjectionHead\n\n#pretraining of BarlowTwins MODEL\nfrom lightly.loss import BarlowTwinsLoss\nfrom lightly.models.modules import BarlowTwinsProjectionHead\nfrom lightly.transforms.byol_transform import (\n    BYOLTransform,\n    BYOLView1Transform,\n    BYOLView2Transform,\n)\n\n\n#fine-tuning the models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport pytorch_lightning as pl\nimport torchvision\nfrom torchvision import transforms\n\n\nfrom pytorch_lightning import seed_everything\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nseed_value = 42\n\ntorch.manual_seed(seed_value)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed_value)\n    \nseed_everything(seed_value) ","metadata":{"execution":{"iopub.status.busy":"2023-11-26T08:43:25.727330Z","iopub.execute_input":"2023-11-26T08:43:25.727643Z","iopub.status.idle":"2023-11-26T08:43:43.851032Z","shell.execute_reply.started":"2023-11-26T08:43:25.727617Z","shell.execute_reply":"2023-11-26T08:43:43.850018Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}]},{"cell_type":"markdown","source":"GETTING ALL THE NATURAL IMAGES TO DO THE PRETRAINING ON","metadata":{}},{"cell_type":"code","source":"imagenet_path='/kaggle/input/tinyimagenet200/tiny-imagenet-200/train'\nfinal_loc='/kaggle/data'\nc=0\n\n#os.makedirs(final_loc, exist_ok=True)\nos.makedirs(final_loc)\n\nfor directory in os.listdir(imagenet_path):\n    c+=1\n    directory_loc=os.path.join(imagenet_path,directory,'images')\n    for file in os.listdir(directory_loc):\n        \n        OLD_PATH=os.path.join(directory_loc,file)\n        NEW_PATH=os.path.join(final_loc,file)\n        #os.replace(OLD_PATH,NEW_PATH)\n        #print(OLD_PATH, NEW_PATH)\n        \n        \n        shutil.copy(OLD_PATH, NEW_PATH)\n        \n   \n    if(c%20==0):\n        print(f'{c} DIRECTORIES TRANSFERED')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T08:43:43.852233Z","iopub.execute_input":"2023-11-26T08:43:43.852514Z","iopub.status.idle":"2023-11-26T08:54:31.409290Z","shell.execute_reply.started":"2023-11-26T08:43:43.852490Z","shell.execute_reply":"2023-11-26T08:54:31.408271Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"20 DIRECTORIES TRANSFERED\n40 DIRECTORIES TRANSFERED\n60 DIRECTORIES TRANSFERED\n80 DIRECTORIES TRANSFERED\n100 DIRECTORIES TRANSFERED\n120 DIRECTORIES TRANSFERED\n140 DIRECTORIES TRANSFERED\n160 DIRECTORIES TRANSFERED\n180 DIRECTORIES TRANSFERED\n200 DIRECTORIES TRANSFERED\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**DEFINING THE SIMCLR PRE-TRAINING PIPELINE**","metadata":{}},{"cell_type":"code","source":"input_size=25\nSC_Imagenet_transform = SimCLRTransform(input_size=input_size)\n\n# Create a dataset from your image folder.\nSC_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SC_Imagenet_transform,\n)\n\n# Build a PyTorch dataloader.\nSC_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SC_Imagenet_dataset,                # Pass the dataset to the dataloader.\n    batch_size=16,         # A large batch size helps with learning.\n    shuffle=True,           # Shuffling is important!\n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T08:54:31.412409Z","iopub.execute_input":"2023-11-26T08:54:31.413091Z","iopub.status.idle":"2023-11-26T08:54:31.953604Z","shell.execute_reply.started":"2023-11-26T08:54:31.413053Z","shell.execute_reply":"2023-11-26T08:54:31.952723Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_epochs=10\n\nclass SimCLR(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n        \n        self.projection_head = SimCLRProjectionHead(512,512,2048)\n        #1st parameter -> input size (from the last layer of the resnet backbone)\n        #2nd parameter -> hidden size\n        #3rd parameter -> output size\n\n        self.criterion = NTXentLoss()\n\n    def forward(self, x):\n        h = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(h)\n        return z\n\n    def training_step(self, batch, batch_idx):\n        (x0, x1), _, _ = batch\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(\n            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n        )\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n        return [optim],[scheduler]\n    \ntorch.cuda.empty_cache()\nSimCLR_ImagenetPT_model = SimCLR()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SimCLR_ImagenetPT_model, SC_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T08:54:31.954904Z","iopub.execute_input":"2023-11-26T08:54:31.955257Z","iopub.status.idle":"2023-11-26T09:31:08.700895Z","shell.execute_reply.started":"2023-11-26T08:54:31.955228Z","shell.execute_reply":"2023-11-26T09:31:08.699760Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7368445e8c0437a850da409a7a24172"}},"metadata":{}}]},{"cell_type":"code","source":"SimCLR_ImagenetPT_model.backbone","metadata":{"execution":{"iopub.status.busy":"2023-11-26T09:31:08.702854Z","iopub.execute_input":"2023-11-26T09:31:08.703242Z","iopub.status.idle":"2023-11-26T09:31:08.713126Z","shell.execute_reply.started":"2023-11-26T09:31:08.703201Z","shell.execute_reply":"2023-11-26T09:31:08.712096Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (5): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (6): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (7): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"**GOING FOR THE BARLOW TWINS MODEL NOW**","metadata":{}},{"cell_type":"code","source":"BT_Imagenet_transform = BYOLTransform(\n    view_1_transform=BYOLView1Transform(input_size=32, gaussian_blur=0.0),\n    view_2_transform=BYOLView2Transform(input_size=32, gaussian_blur=0.0),\n)\n\nBT_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=BT_Imagenet_transform,\n)\n\nBT_Imagenet_dataloader = torch.utils.data.DataLoader(\n    BT_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T09:31:08.714360Z","iopub.execute_input":"2023-11-26T09:31:08.714736Z","iopub.status.idle":"2023-11-26T09:31:09.329709Z","shell.execute_reply.started":"2023-11-26T09:31:08.714701Z","shell.execute_reply":"2023-11-26T09:31:09.328588Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"max_epochs=10\n\nclass BarlowTwins(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n        self.criterion = BarlowTwinsLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(x)\n        return z\n\n    def training_step(self, batch, batch_index):\n        (x0, x1) = batch[0]\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n        return optim\n        \ntorch.cuda.empty_cache()\nBT_ImagenetPT_model = BarlowTwins()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(BT_ImagenetPT_model, BT_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T09:31:09.331473Z","iopub.execute_input":"2023-11-26T09:31:09.331893Z","iopub.status.idle":"2023-11-26T10:06:12.090545Z","shell.execute_reply.started":"2023-11-26T09:31:09.331855Z","shell.execute_reply":"2023-11-26T10:06:12.089375Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ae80a9090be49b4811ebc529fefdd10"}},"metadata":{}}]},{"cell_type":"markdown","source":"**PRETRAINING OVER SWAV MODEL NOW**","metadata":{}},{"cell_type":"code","source":"from lightly.loss import SwaVLoss\nfrom lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\nfrom lightly.transforms.swav_transform import SwaVTransform\n\n\nSW_Imagenet_transform = SwaVTransform()\n# we ignore object detection annotations by setting target_transform to return 0\nSW_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SW_Imagenet_transform,\n)\n# or create a dataset from a folder containing images or videos:\n# dataset = LightlyDataset(\"path/to/folder\")\n\nSW_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SW_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:06:12.092356Z","iopub.execute_input":"2023-11-26T10:06:12.092688Z","iopub.status.idle":"2023-11-26T10:06:12.628558Z","shell.execute_reply.started":"2023-11-26T10:06:12.092660Z","shell.execute_reply":"2023-11-26T10:06:12.627567Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_epochs=6\nclass SwaV(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = SwaVProjectionHead(512, 512, 128)\n        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n        self.criterion = SwaVLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        x = self.projection_head(x)\n        x = nn.functional.normalize(x, dim=1, p=2)\n        p = self.prototypes(x)\n        return p\n\n    def training_step(self, batch, batch_idx):\n        self.prototypes.normalize()\n        views = batch[0]\n        multi_crop_features = [self.forward(view.to(self.device)) for view in views]\n        high_resolution = multi_crop_features[:2]\n        low_resolution =  multi_crop_features[2:]\n        loss = self.criterion(high_resolution, low_resolution)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optim\n\ntorch.cuda.empty_cache()\nSW_ImagenetPT_model = SwaV()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SW_ImagenetPT_model, SW_Imagenet_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:32.360831Z","iopub.execute_input":"2023-11-26T10:30:32.363215Z","iopub.status.idle":"2023-11-26T12:42:13.847884Z","shell.execute_reply.started":"2023-11-26T10:30:32.363171Z","shell.execute_reply":"2023-11-26T12:42:13.846913Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b077e47a9ee74ee59ed70457ed0da3d9"}},"metadata":{}}]},{"cell_type":"markdown","source":"**PREPARING THE DATA FOR THE FINE-TUNING OF THESE PRETRAINED MODELS**","metadata":{}},{"cell_type":"code","source":"def one_hot(n):\n    index={0:0,0.5:1,1:2,2:3}\n    #print(index)\n    #print(index[n])\n    y_new=np.zeros(4)\n    y_new[index[n]]=1\n    #print(y_new)\n    return y_new","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:13.854168Z","iopub.execute_input":"2023-11-26T12:42:13.854488Z","iopub.status.idle":"2023-11-26T12:42:13.860063Z","shell.execute_reply.started":"2023-11-26T12:42:13.854452Z","shell.execute_reply":"2023-11-26T12:42:13.859057Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"c=0\ndf_new = pd.DataFrame(columns=['IMG', 'CDR'])\ndef make_dataset(path,cdr):\n    \n    global c, df_new\n    \n    for i in os.listdir(path):\n        c+=1\n        image = cv2.imread(os.path.join(path,i))\n        image_np = np.array(image)\n        image_tensor = transforms.ToTensor()(image_np)\n        padded_image = transforms.CenterCrop((224, 224))(transforms.Pad(28)(image_tensor))\n        #print(padded_image.shape)\n        '''plt.imshow(padded_image.permute(1, 2, 0))\n        print('padded_image',padded_image.shape)'''  \n        \n        new_entry = {\n            'IMG': padded_image,\n            'CDR': cdr\n        } \n        df_new = pd.concat([df_new, pd.DataFrame([new_entry])], ignore_index=True)\n        #print(i)\n\n\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented',0.5)\n        \nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/VeryMildDemented',0.5)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:13.861164Z","iopub.execute_input":"2023-11-26T12:42:13.861462Z","iopub.status.idle":"2023-11-26T12:42:54.584994Z","shell.execute_reply.started":"2023-11-26T12:42:13.861409Z","shell.execute_reply":"2023-11-26T12:42:54.583976Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(c,df_new.shape)\nprint(df_new['IMG'][0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:54.586189Z","iopub.execute_input":"2023-11-26T12:42:54.586503Z","iopub.status.idle":"2023-11-26T12:42:54.596748Z","shell.execute_reply.started":"2023-11-26T12:42:54.586477Z","shell.execute_reply":"2023-11-26T12:42:54.595755Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"6400 (6400, 2)\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"x=np.stack(df_new['IMG'].tolist())\ny=df_new['CDR']\n\ny_one_hot=[]\nfor i in y:\n    y_one_hot.append(one_hot(i)) ","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:54.597812Z","iopub.execute_input":"2023-11-26T12:42:54.598069Z","iopub.status.idle":"2023-11-26T12:42:55.818915Z","shell.execute_reply.started":"2023-11-26T12:42:54.598047Z","shell.execute_reply":"2023-11-26T12:42:55.818111Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(type(x))","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:55.820204Z","iopub.execute_input":"2023-11-26T12:42:55.820501Z","iopub.status.idle":"2023-11-26T12:42:55.825618Z","shell.execute_reply.started":"2023-11-26T12:42:55.820477Z","shell.execute_reply":"2023-11-26T12:42:55.824710Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"x=torch.tensor(x)\ny=torch.Tensor(y_one_hot)\n\ndataset=TensorDataset(x,y)\n\ntrain_size=int(0.6*len(x))\nval_size=int(0.2*len(x))\ntemp_size=2*val_size\n\n#print(train_size,val_size,temp_size)\n\ntrain_dataset, temp_dataset = random_split(dataset, [train_size, temp_size])\nval_dataset,test_dataset=random_split(temp_dataset, [val_size,val_size])\n\n# Define batch size\n#batch_size = 8\n\n# Create a DataLoader for shuffling and batching\n#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:55.826935Z","iopub.execute_input":"2023-11-26T12:42:55.827276Z","iopub.status.idle":"2023-11-26T12:42:57.153823Z","shell.execute_reply.started":"2023-11-26T12:42:55.827235Z","shell.execute_reply":"2023-11-26T12:42:57.152859Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/701760665.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  y=torch.Tensor(y_one_hot)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**DEFINING FUNCTIONS TO PERFORM THE FINE-TUNING ON DIFFERENT MODELS**","metadata":{}},{"cell_type":"code","source":"def train_and_val(backbone):\n    \n    #training the models first on train_dataloader\n    \n    models=[]\n    for i in range(2):\n        models.append(ResnetSingleChannel(backbone,4))\n        \n   \n    \n    batch_sizes=[64,32]\n    \n    for model_no in range(2):\n        print(f'TRAINING MODEL {model_no+1}')\n        print('--------------------------------------------------------------')\n        fine_tune_opt = optim.Adam(models[model_no].parameters(), lr=0.001, weight_decay=0.0001)\n        ft_loss_fn= nn.CrossEntropyLoss()\n        \n        batch_size=batch_sizes[model_no]\n        \n        train_dataloader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n        \n\n        num_epochs = 5\n        total_steps=train_size//batch_size\n\n        for epoch in range(num_epochs):\n            i=0\n\n            for x_batch,y_batch in train_dataloader:\n                i+=1\n                outputs=models[model_no](torch.Tensor(x_batch))\n                \n                '''print('out',outputs.shape)\n                print('y',y_batch.shape)\n                print('out',outputs)\n                print('y',y_batch)'''\n                \n                loss = ft_loss_fn(outputs, y_batch)\n                fine_tune_opt.zero_grad()\n                loss.backward()\n                fine_tune_opt.step()\n\n                if (i+1) % 20 == 0:\n                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n    \n    # performing validation on validation data_loader\n    best_model=-1\n    best_loss=np.inf\n    for model_no in range(2):\n        \n        loss_sum=0\n        \n        for x_batch,y_batch in val_dataloader:\n            outputs=models[model_no](torch.Tensor(x_batch))\n            loss = ft_loss_fn(outputs, y_batch)\n            loss_sum+=loss.item()\n            \n        if(loss_sum<best_loss):\n            best_model=model_no\n            best_loss=loss_sum\n    \n    return models[best_model]\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:57.155202Z","iopub.execute_input":"2023-11-26T12:42:57.155920Z","iopub.status.idle":"2023-11-26T12:42:57.167305Z","shell.execute_reply.started":"2023-11-26T12:42:57.155884Z","shell.execute_reply":"2023-11-26T12:42:57.166453Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def metrics(model):\n    \n    test_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)\n    \n    y_pred_M=[]\n    y_true_M=[]\n    c=0\n    logloss_sum=0\n    \n    for x_batch,y_batch in test_dataloader:\n        \n        c+=1\n        y_pred=model(x_batch)\n\n        '''y_pred_M.append(np.argmax(y_pred.detach().numpy()))\n        y_true_M.append(np.argmax(y_batch.detach().numpy()))'''\n\n        softmax_probs = softmax(y_pred)\n\n        '''print(y_pred)\n        print(softmax_probs)\n        print(y_batch)'''\n\n        logloss = log_loss(y_batch.detach().numpy(), softmax_probs.detach().numpy())\n\n        logloss_sum+=logloss\n        #print(logloss) \n\n    return(logloss_sum/c)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T14:04:08.245331Z","iopub.execute_input":"2023-11-26T14:04:08.246075Z","iopub.status.idle":"2023-11-26T14:04:08.252571Z","shell.execute_reply.started":"2023-11-26T14:04:08.246041Z","shell.execute_reply":"2023-11-26T14:04:08.251649Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#MAKING A NEW NETWORK WHICH USES THE BACKBONE OF THE PRETRAINED MODEL, AND HAS AN INPUT LAYER TO TAKE IN A SINGLE CHANNEL IMAGE, AND THEN PASS INTO \n#BACKBONE, AND FINALLY OUTPUT LAYER WHICH PERFORMS REGRESSION\n\nclass ResnetSingleChannel(nn.Module):\n    def __init__(self,backbone,channels):\n        super(ResnetSingleChannel,self).__init__()\n        self.input=nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1, bias=False)\n        self.backbone=backbone\n        self.output=nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.Linear(in_features=256, out_features=channels)\n        )\n        \n    def forward(self,x):\n        x=self.input(x)\n        x=self.backbone(x)\n        x=self.output(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:57.183090Z","iopub.execute_input":"2023-11-26T12:42:57.183356Z","iopub.status.idle":"2023-11-26T12:42:57.193841Z","shell.execute_reply.started":"2023-11-26T12:42:57.183332Z","shell.execute_reply":"2023-11-26T12:42:57.192965Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**FINE TUNING OVER THE SIMCLR MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in SimCLR_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSC_MODEL=train_and_val(SimCLR_ImagenetPT_model.backbone)\nSC_M_2=metrics(SC_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T12:42:57.194770Z","iopub.execute_input":"2023-11-26T12:42:57.195041Z","iopub.status.idle":"2023-11-26T13:33:14.038720Z","shell.execute_reply.started":"2023-11-26T12:42:57.195018Z","shell.execute_reply":"2023-11-26T13:33:14.037649Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/5], Step [20/60], Loss: 0.8923\nEpoch [1/5], Step [40/60], Loss: 0.7936\nEpoch [1/5], Step [60/60], Loss: 0.8855\nEpoch [2/5], Step [20/60], Loss: 0.8399\nEpoch [2/5], Step [40/60], Loss: 0.7186\nEpoch [2/5], Step [60/60], Loss: 0.7142\nEpoch [3/5], Step [20/60], Loss: 0.6325\nEpoch [3/5], Step [40/60], Loss: 0.5953\nEpoch [3/5], Step [60/60], Loss: 0.4949\nEpoch [4/5], Step [20/60], Loss: 0.3282\nEpoch [4/5], Step [40/60], Loss: 0.4406\nEpoch [4/5], Step [60/60], Loss: 0.2990\nEpoch [5/5], Step [20/60], Loss: 0.2853\nEpoch [5/5], Step [40/60], Loss: 0.1821\nEpoch [5/5], Step [60/60], Loss: 0.3797\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/5], Step [20/120], Loss: 0.9007\nEpoch [1/5], Step [40/120], Loss: 0.8856\nEpoch [1/5], Step [60/120], Loss: 0.7238\nEpoch [1/5], Step [80/120], Loss: 0.9419\nEpoch [1/5], Step [100/120], Loss: 0.7721\nEpoch [1/5], Step [120/120], Loss: 0.7555\nEpoch [2/5], Step [20/120], Loss: 0.6923\nEpoch [2/5], Step [40/120], Loss: 1.0350\nEpoch [2/5], Step [60/120], Loss: 0.9484\nEpoch [2/5], Step [80/120], Loss: 0.7326\nEpoch [2/5], Step [100/120], Loss: 0.8798\nEpoch [2/5], Step [120/120], Loss: 0.6086\nEpoch [3/5], Step [20/120], Loss: 0.7004\nEpoch [3/5], Step [40/120], Loss: 0.5648\nEpoch [3/5], Step [60/120], Loss: 0.4513\nEpoch [3/5], Step [80/120], Loss: 0.6151\nEpoch [3/5], Step [100/120], Loss: 0.6567\nEpoch [3/5], Step [120/120], Loss: 0.4116\nEpoch [4/5], Step [20/120], Loss: 0.3213\nEpoch [4/5], Step [40/120], Loss: 0.2714\nEpoch [4/5], Step [60/120], Loss: 0.7240\nEpoch [4/5], Step [80/120], Loss: 0.2604\nEpoch [4/5], Step [100/120], Loss: 0.6271\nEpoch [4/5], Step [120/120], Loss: 0.5089\nEpoch [5/5], Step [20/120], Loss: 0.2039\nEpoch [5/5], Step [40/120], Loss: 0.1715\nEpoch [5/5], Step [60/120], Loss: 0.1861\nEpoch [5/5], Step [80/120], Loss: 0.1488\nEpoch [5/5], Step [100/120], Loss: 0.2359\nEpoch [5/5], Step [120/120], Loss: 0.1124\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**FINE TUNING OVER THE BARLOW TWINS MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in BT_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nBT_MODEL=train_and_val(BT_ImagenetPT_model.backbone)\nBT_M_2=metrics(BT_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T14:10:51.748140Z","iopub.execute_input":"2023-11-26T14:10:51.748828Z","iopub.status.idle":"2023-11-26T15:02:59.704655Z","shell.execute_reply.started":"2023-11-26T14:10:51.748787Z","shell.execute_reply":"2023-11-26T15:02:59.703611Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/5], Step [20/60], Loss: 1.7632\nEpoch [1/5], Step [40/60], Loss: 1.2376\nEpoch [1/5], Step [60/60], Loss: 1.2612\nEpoch [2/5], Step [20/60], Loss: 0.9757\nEpoch [2/5], Step [40/60], Loss: 0.9629\nEpoch [2/5], Step [60/60], Loss: 1.0756\nEpoch [3/5], Step [20/60], Loss: 1.0650\nEpoch [3/5], Step [40/60], Loss: 0.8951\nEpoch [3/5], Step [60/60], Loss: 0.8176\nEpoch [4/5], Step [20/60], Loss: 0.8993\nEpoch [4/5], Step [40/60], Loss: 1.1381\nEpoch [4/5], Step [60/60], Loss: 1.0183\nEpoch [5/5], Step [20/60], Loss: 0.9652\nEpoch [5/5], Step [40/60], Loss: 0.9370\nEpoch [5/5], Step [60/60], Loss: 0.8118\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/5], Step [20/120], Loss: 1.3414\nEpoch [1/5], Step [40/120], Loss: 2.6125\nEpoch [1/5], Step [60/120], Loss: 2.0248\nEpoch [1/5], Step [80/120], Loss: 1.2361\nEpoch [1/5], Step [100/120], Loss: 0.8813\nEpoch [1/5], Step [120/120], Loss: 0.8750\nEpoch [2/5], Step [20/120], Loss: 1.4347\nEpoch [2/5], Step [40/120], Loss: 1.0594\nEpoch [2/5], Step [60/120], Loss: 1.0987\nEpoch [2/5], Step [80/120], Loss: 0.9590\nEpoch [2/5], Step [100/120], Loss: 0.7845\nEpoch [2/5], Step [120/120], Loss: 1.1220\nEpoch [3/5], Step [20/120], Loss: 1.0610\nEpoch [3/5], Step [40/120], Loss: 1.0205\nEpoch [3/5], Step [60/120], Loss: 0.8284\nEpoch [3/5], Step [80/120], Loss: 0.8204\nEpoch [3/5], Step [100/120], Loss: 0.9490\nEpoch [3/5], Step [120/120], Loss: 0.9392\nEpoch [4/5], Step [20/120], Loss: 0.8634\nEpoch [4/5], Step [40/120], Loss: 1.1983\nEpoch [4/5], Step [60/120], Loss: 1.3458\nEpoch [4/5], Step [80/120], Loss: 1.0568\nEpoch [4/5], Step [100/120], Loss: 1.1271\nEpoch [4/5], Step [120/120], Loss: 1.0819\nEpoch [5/5], Step [20/120], Loss: 1.4475\nEpoch [5/5], Step [40/120], Loss: 1.0457\nEpoch [5/5], Step [60/120], Loss: 1.3499\nEpoch [5/5], Step [80/120], Loss: 0.9417\nEpoch [5/5], Step [100/120], Loss: 0.8086\nEpoch [5/5], Step [120/120], Loss: 0.9784\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**FINE TUNING OVER THE SWAV MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in SW_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSW_MODEL=train_and_val(SW_ImagenetPT_model.backbone)\nSW_M_2=metrics(SW_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:02:59.706751Z","iopub.execute_input":"2023-11-26T15:02:59.707083Z","iopub.status.idle":"2023-11-26T15:55:58.789164Z","shell.execute_reply.started":"2023-11-26T15:02:59.707055Z","shell.execute_reply":"2023-11-26T15:55:58.788268Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/5], Step [20/60], Loss: 0.9210\nEpoch [1/5], Step [40/60], Loss: 0.9176\nEpoch [1/5], Step [60/60], Loss: 0.9756\nEpoch [2/5], Step [20/60], Loss: 1.1060\nEpoch [2/5], Step [40/60], Loss: 0.9630\nEpoch [2/5], Step [60/60], Loss: 0.7968\nEpoch [3/5], Step [20/60], Loss: 0.8326\nEpoch [3/5], Step [40/60], Loss: 0.8409\nEpoch [3/5], Step [60/60], Loss: 0.8075\nEpoch [4/5], Step [20/60], Loss: 0.8612\nEpoch [4/5], Step [40/60], Loss: 0.8012\nEpoch [4/5], Step [60/60], Loss: 0.8534\nEpoch [5/5], Step [20/60], Loss: 0.8718\nEpoch [5/5], Step [40/60], Loss: 0.8418\nEpoch [5/5], Step [60/60], Loss: 0.9419\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/5], Step [20/120], Loss: 1.0153\nEpoch [1/5], Step [40/120], Loss: 1.0145\nEpoch [1/5], Step [60/120], Loss: 0.7883\nEpoch [1/5], Step [80/120], Loss: 0.7911\nEpoch [1/5], Step [100/120], Loss: 0.8555\nEpoch [1/5], Step [120/120], Loss: 1.2656\nEpoch [2/5], Step [20/120], Loss: 0.8514\nEpoch [2/5], Step [40/120], Loss: 1.0147\nEpoch [2/5], Step [60/120], Loss: 0.8386\nEpoch [2/5], Step [80/120], Loss: 0.8896\nEpoch [2/5], Step [100/120], Loss: 0.9189\nEpoch [2/5], Step [120/120], Loss: 0.8626\nEpoch [3/5], Step [20/120], Loss: 0.7918\nEpoch [3/5], Step [40/120], Loss: 1.0528\nEpoch [3/5], Step [60/120], Loss: 0.8860\nEpoch [3/5], Step [80/120], Loss: 0.8149\nEpoch [3/5], Step [100/120], Loss: 1.1563\nEpoch [3/5], Step [120/120], Loss: 0.8560\nEpoch [4/5], Step [20/120], Loss: 0.9823\nEpoch [4/5], Step [40/120], Loss: 1.0296\nEpoch [4/5], Step [60/120], Loss: 0.9183\nEpoch [4/5], Step [80/120], Loss: 0.7813\nEpoch [4/5], Step [100/120], Loss: 0.7742\nEpoch [4/5], Step [120/120], Loss: 0.7756\nEpoch [5/5], Step [20/120], Loss: 0.9738\nEpoch [5/5], Step [40/120], Loss: 0.8112\nEpoch [5/5], Step [60/120], Loss: 0.7575\nEpoch [5/5], Step [80/120], Loss: 0.8637\nEpoch [5/5], Step [100/120], Loss: 0.8153\nEpoch [5/5], Step [120/120], Loss: 0.6939\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**FINE TUNING OVER THE UNTRAINED RESNET MODEL**","metadata":{}},{"cell_type":"code","source":"resnet = torchvision.models.resnet18()\nuntrained= nn.Sequential(*list(resnet.children())[:-1])\n\nfor layer in untrained:\n    for param in layer.parameters():\n        param.requires_grad = True\n\nUN_MODEL=train_and_val(untrained)\nUN_M_2=metrics(UN_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:55:58.790689Z","iopub.execute_input":"2023-11-26T15:55:58.790992Z","iopub.status.idle":"2023-11-26T16:49:31.002335Z","shell.execute_reply.started":"2023-11-26T15:55:58.790966Z","shell.execute_reply":"2023-11-26T16:49:31.001223Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/5], Step [20/60], Loss: 0.9472\nEpoch [1/5], Step [40/60], Loss: 0.9067\nEpoch [1/5], Step [60/60], Loss: 0.8416\nEpoch [2/5], Step [20/60], Loss: 0.8884\nEpoch [2/5], Step [40/60], Loss: 0.9664\nEpoch [2/5], Step [60/60], Loss: 0.6954\nEpoch [3/5], Step [20/60], Loss: 0.7631\nEpoch [3/5], Step [40/60], Loss: 0.7042\nEpoch [3/5], Step [60/60], Loss: 0.7734\nEpoch [4/5], Step [20/60], Loss: 0.5689\nEpoch [4/5], Step [40/60], Loss: 0.5364\nEpoch [4/5], Step [60/60], Loss: 0.4203\nEpoch [5/5], Step [20/60], Loss: 0.3297\nEpoch [5/5], Step [40/60], Loss: 0.5171\nEpoch [5/5], Step [60/60], Loss: 0.4068\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/5], Step [20/120], Loss: 0.2892\nEpoch [1/5], Step [40/120], Loss: 0.6161\nEpoch [1/5], Step [60/120], Loss: 0.4237\nEpoch [1/5], Step [80/120], Loss: 0.4180\nEpoch [1/5], Step [100/120], Loss: 0.3245\nEpoch [1/5], Step [120/120], Loss: 0.8100\nEpoch [2/5], Step [20/120], Loss: 0.1765\nEpoch [2/5], Step [40/120], Loss: 0.3272\nEpoch [2/5], Step [60/120], Loss: 0.3934\nEpoch [2/5], Step [80/120], Loss: 0.1763\nEpoch [2/5], Step [100/120], Loss: 0.4254\nEpoch [2/5], Step [120/120], Loss: 0.5027\nEpoch [3/5], Step [20/120], Loss: 0.1229\nEpoch [3/5], Step [40/120], Loss: 0.4423\nEpoch [3/5], Step [60/120], Loss: 0.1265\nEpoch [3/5], Step [80/120], Loss: 0.2385\nEpoch [3/5], Step [100/120], Loss: 0.6127\nEpoch [3/5], Step [120/120], Loss: 0.2976\nEpoch [4/5], Step [20/120], Loss: 0.0215\nEpoch [4/5], Step [40/120], Loss: 0.0828\nEpoch [4/5], Step [60/120], Loss: 0.2686\nEpoch [4/5], Step [80/120], Loss: 0.0929\nEpoch [4/5], Step [100/120], Loss: 0.1930\nEpoch [4/5], Step [120/120], Loss: 0.4118\nEpoch [5/5], Step [20/120], Loss: 0.1222\nEpoch [5/5], Step [40/120], Loss: 0.0156\nEpoch [5/5], Step [60/120], Loss: 0.2249\nEpoch [5/5], Step [80/120], Loss: 0.1602\nEpoch [5/5], Step [100/120], Loss: 0.0533\nEpoch [5/5], Step [120/120], Loss: 0.2150\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PRINTING OUT THE PERFORMANCE METRICS**","metadata":{}},{"cell_type":"code","source":"print('SimCLR',SC_M_2)\nprint('Barlow',BT_M_2)\nprint('Swav',SW_M_2)\nprint('Untrained',UN_M_2)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T17:26:54.783039Z","iopub.execute_input":"2023-11-26T17:26:54.783455Z","iopub.status.idle":"2023-11-26T17:26:54.789044Z","shell.execute_reply.started":"2023-11-26T17:26:54.783412Z","shell.execute_reply":"2023-11-26T17:26:54.788035Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"SimCLR 1.4556029397360444\nBarlow 0.9936796108116001\nSwav 1.0323936266081515\nUntrained 1.0058041749604016\n","output_type":"stream"}]}]}