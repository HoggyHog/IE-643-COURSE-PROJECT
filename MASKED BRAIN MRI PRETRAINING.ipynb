{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":861496,"sourceType":"datasetVersion","datasetId":457093},{"sourceId":7057382,"sourceType":"datasetVersion","datasetId":4062415}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**INSTALLING THE REQUIRED DEPENDENCIES**","metadata":{}},{"cell_type":"code","source":"!pip install lightly -q","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:18:14.767376Z","iopub.execute_input":"2023-11-27T01:18:14.767722Z","iopub.status.idle":"2023-11-27T01:18:32.679225Z","shell.execute_reply.started":"2023-11-27T01:18:14.767688Z","shell.execute_reply":"2023-11-27T01:18:32.678038Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**IMPORTING THE REQUIRED DEPENDENCIES**","metadata":{}},{"cell_type":"code","source":"#dependencies for file management and data visualisation\nimport os\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nibabel as nib\nimport re\nfrom tqdm import tqdm\nimport pandas as pd\nimport gc\nimport psutil\nfrom matplotlib.animation import FuncAnimation\nimport seaborn as sns\nfrom IPython.display import HTML\nimport cv2\n\n#pretraining dependencies\nfrom lightly.data import LightlyDataset\n\n#pretraining of SIMCLR MODEL\nfrom lightly.transforms.simclr_transform import SimCLRTransform\nfrom lightly.loss import NTXentLoss\nfrom lightly.models.modules.heads import SimCLRProjectionHead\n\n#pretraining of BarlowTwins MODEL\nfrom lightly.loss import BarlowTwinsLoss\nfrom lightly.models.modules import BarlowTwinsProjectionHead\nfrom lightly.transforms.byol_transform import (\n    BYOLTransform,\n    BYOLView1Transform,\n    BYOLView2Transform,\n)\n\n#pretraining on SWaV MODEL\nfrom lightly.loss import SwaVLoss\nfrom lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\nfrom lightly.transforms.swav_transform import SwaVTransform\n\n\n#fine-tuning the models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader , random_split\nimport pytorch_lightning as pl\nimport torchvision\nfrom torchvision import transforms\nfrom pytorch_lightning import seed_everything\n\n#evaluating the models\nfrom sklearn.metrics import log_loss\n\nseed_value = 50\n\ntorch.manual_seed(seed_value)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed_value)\n\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:18:32.681106Z","iopub.execute_input":"2023-11-27T01:18:32.681412Z","iopub.status.idle":"2023-11-27T01:18:40.428996Z","shell.execute_reply.started":"2023-11-27T01:18:32.681383Z","shell.execute_reply":"2023-11-27T01:18:40.427954Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}]},{"cell_type":"markdown","source":"**GETTING ALL THE MRI MASK IMAGES INTO ONE SINGLE FOLDER**","metadata":{}},{"cell_type":"code","source":"final_loc='/kaggle/data'\nc=0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T01:18:40.430106Z","iopub.execute_input":"2023-11-27T01:18:40.430588Z","iopub.status.idle":"2023-11-27T01:18:40.434584Z","shell.execute_reply.started":"2023-11-27T01:18:40.430562Z","shell.execute_reply":"2023-11-27T01:18:40.433697Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data='/kaggle/input/brain-mri-masks/images'\n\nos.makedirs(final_loc,exist_ok=True)\nfor file in os.listdir(data):\n    c+=1\n    \n    file_path=os.path.join(data,file)\n    ext=file[-4:]\n    if(ext=='jpeg'):\n        ext='.jpeg'\n    new_filename=str(c)+ext\n    final_path=os.path.join(final_loc,new_filename)\n    \n    shutil.copy(file_path, final_path)\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:18:40.436537Z","iopub.execute_input":"2023-11-27T01:18:40.436813Z","iopub.status.idle":"2023-11-27T01:19:05.875853Z","shell.execute_reply.started":"2023-11-27T01:18:40.436789Z","shell.execute_reply":"2023-11-27T01:19:05.874854Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"counter=0\nfor i in os.listdir(final_loc):\n    counter+=1\nprint(c,counter)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:19:05.877339Z","iopub.execute_input":"2023-11-27T01:19:05.877709Z","iopub.status.idle":"2023-11-27T01:19:05.895751Z","shell.execute_reply.started":"2023-11-27T01:19:05.877675Z","shell.execute_reply":"2023-11-27T01:19:05.894724Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"7858 7858\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**MAKING THE SIMCLR MODEL NOW**","metadata":{}},{"cell_type":"code","source":"input_size=32\nSC_Imagenet_transform = SimCLRTransform(input_size=input_size)\n\n# Create a dataset from your image folder.\nSC_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SC_Imagenet_transform,\n)\n\n# Build a PyTorch dataloader.\nSC_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SC_Imagenet_dataset,                # Pass the dataset to the dataloader.\n    batch_size=16,         # A large batch size helps with learning.\n    shuffle=True,           # Shuffling is important!\n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:19:05.899745Z","iopub.execute_input":"2023-11-27T01:19:05.900994Z","iopub.status.idle":"2023-11-27T01:19:05.949425Z","shell.execute_reply.started":"2023-11-27T01:19:05.900952Z","shell.execute_reply":"2023-11-27T01:19:05.948400Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_epochs=15\n\nclass SimCLR(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n        \n        self.projection_head = SimCLRProjectionHead(512,512,2048)\n        #1st parameter -> input size (from the last layer of the resnet backbone)\n        #2nd parameter -> hidden size\n        #3rd parameter -> output size\n\n        self.criterion = NTXentLoss()\n\n    def forward(self, x):\n        h = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(h)\n        return z\n\n    def training_step(self, batch, batch_idx):\n        (x0, x1), _, _ = batch\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(\n            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n        )\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n        return [optim],[scheduler]\n    \ntorch.cuda.empty_cache()\nSimCLR_ImagenetPT_model = SimCLR()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SimCLR_ImagenetPT_model, SC_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:19:05.950757Z","iopub.execute_input":"2023-11-27T01:19:05.951113Z","iopub.status.idle":"2023-11-27T01:24:28.935155Z","shell.execute_reply.started":"2023-11-27T01:19:05.951084Z","shell.execute_reply":"2023-11-27T01:24:28.934248Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93381f3af41440b9a6c8e55eb4a3748"}},"metadata":{}}]},{"cell_type":"markdown","source":"**MAKING THE BARLOWTWINS MODEL NOW**","metadata":{}},{"cell_type":"code","source":"BT_Imagenet_transform = BYOLTransform(\n    view_1_transform=BYOLView1Transform(input_size=32, gaussian_blur=0.0),\n    view_2_transform=BYOLView2Transform(input_size=32, gaussian_blur=0.0),\n)\n\nBT_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=BT_Imagenet_transform,\n)\n\nBT_Imagenet_dataloader = torch.utils.data.DataLoader(\n    BT_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:24:28.936648Z","iopub.execute_input":"2023-11-27T01:24:28.936969Z","iopub.status.idle":"2023-11-27T01:24:28.985067Z","shell.execute_reply.started":"2023-11-27T01:24:28.936938Z","shell.execute_reply":"2023-11-27T01:24:28.984295Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"max_epochs=15\n\nclass BarlowTwins(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n        self.criterion = BarlowTwinsLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(x)\n        return z\n\n    def training_step(self, batch, batch_index):\n        (x0, x1) = batch[0]\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n        return optim\n        \ntorch.cuda.empty_cache()\nBT_ImagenetPT_model = BarlowTwins()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(BT_ImagenetPT_model, BT_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:24:28.986273Z","iopub.execute_input":"2023-11-27T01:24:28.986566Z","iopub.status.idle":"2023-11-27T01:29:09.776803Z","shell.execute_reply.started":"2023-11-27T01:24:28.986542Z","shell.execute_reply":"2023-11-27T01:29:09.775598Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e6bf05a2794b76b7d4ff54acd492d7"}},"metadata":{}}]},{"cell_type":"markdown","source":"**PRETRAINING OVER SWAV MODEL NOW**","metadata":{}},{"cell_type":"code","source":"SW_Imagenet_transform = SwaVTransform()\n# we ignore object detection annotations by setting target_transform to return 0\nSW_Imagenet_dataset = LightlyDataset(\n    input_dir=final_loc,\n    transform=SW_Imagenet_transform,\n)\n# or create a dataset from a folder containing images or videos:\n# dataset = LightlyDataset(\"path/to/folder\")\n\nSW_Imagenet_dataloader = torch.utils.data.DataLoader(\n    SW_Imagenet_dataset,   \n    batch_size=16,         \n    shuffle=True,           \n    num_workers=4\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:29:09.782486Z","iopub.execute_input":"2023-11-27T01:29:09.782905Z","iopub.status.idle":"2023-11-27T01:29:09.834376Z","shell.execute_reply.started":"2023-11-27T01:29:09.782865Z","shell.execute_reply":"2023-11-27T01:29:09.833345Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"max_epochs=10\nclass SwaV(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = SwaVProjectionHead(512, 512, 128)\n        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n        self.criterion = SwaVLoss()\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        x = self.projection_head(x)\n        x = nn.functional.normalize(x, dim=1, p=2)\n        p = self.prototypes(x)\n        return p\n\n    def training_step(self, batch, batch_idx):\n        self.prototypes.normalize()\n        views = batch[0]\n        multi_crop_features = [self.forward(view.to(self.device)) for view in views]\n        high_resolution = multi_crop_features[:2]\n        low_resolution = multi_crop_features[2:]\n        loss = self.criterion(high_resolution, low_resolution)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optim\n\ntorch.cuda.empty_cache()\nSW_ImagenetPT_model = SwaV()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(SW_ImagenetPT_model, SW_Imagenet_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:29:09.835827Z","iopub.execute_input":"2023-11-27T01:29:09.836219Z","iopub.status.idle":"2023-11-27T01:48:39.174118Z","shell.execute_reply.started":"2023-11-27T01:29:09.836183Z","shell.execute_reply":"2023-11-27T01:48:39.172993Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ef07ae48684f6fbf1b831193eb4907"}},"metadata":{}}]},{"cell_type":"markdown","source":"**NOW THAT THE PRETRAINING IS DONE, WE DEFINE THE TRAIN AND TEST DATALOADER FOR THE FINE-TUNING TASK**","metadata":{}},{"cell_type":"code","source":"def one_hot(n):\n    index={0:0,0.5:1,1:2,2:3}\n    #print(index)\n    #print(index[n])\n    y_new=np.zeros(4)\n    y_new[index[n]]=1\n    #print(y_new)\n    return y_new","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:48:39.176139Z","iopub.execute_input":"2023-11-27T01:48:39.177011Z","iopub.status.idle":"2023-11-27T01:48:39.182824Z","shell.execute_reply.started":"2023-11-27T01:48:39.176965Z","shell.execute_reply":"2023-11-27T01:48:39.181842Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"c=0\ndf_new = pd.DataFrame(columns=['IMG', 'CDR'])\ndef make_dataset(path,cdr):\n    \n    global c, df_new\n    \n    for i in os.listdir(path):\n        c+=1\n        image = cv2.imread(os.path.join(path,i))\n        image_np = np.array(image)\n        image_tensor = transforms.ToTensor()(image_np)\n        padded_image = transforms.CenterCrop((224, 224))(transforms.Pad(28)(image_tensor))\n        #print(padded_image.shape)\n        '''plt.imshow(padded_image.permute(1, 2, 0))\n        print('padded_image',padded_image.shape)'''  \n        \n        new_entry = {\n            'IMG': padded_image,\n            'CDR': cdr\n        } \n        df_new = pd.concat([df_new, pd.DataFrame([new_entry])], ignore_index=True)\n        #print(i)\n\n\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented',0.5)\n        \nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/MildDemented',1)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/ModerateDemented',2)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/NonDemented',0)\nmake_dataset('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/VeryMildDemented',0.5)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:48:39.184198Z","iopub.execute_input":"2023-11-27T01:48:39.184807Z","iopub.status.idle":"2023-11-27T01:49:30.462163Z","shell.execute_reply.started":"2023-11-27T01:48:39.184773Z","shell.execute_reply":"2023-11-27T01:49:30.461299Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(c,df_new.shape)\nprint(df_new['IMG'][0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:30.463394Z","iopub.execute_input":"2023-11-27T01:49:30.463712Z","iopub.status.idle":"2023-11-27T01:49:30.473027Z","shell.execute_reply.started":"2023-11-27T01:49:30.463679Z","shell.execute_reply":"2023-11-27T01:49:30.471975Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"6400 (6400, 2)\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"x=np.stack(df_new['IMG'].tolist())\ny=df_new['CDR']\n\ny_one_hot=[]\nfor i in y:\n    y_one_hot.append(one_hot(i)) ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:30.474366Z","iopub.execute_input":"2023-11-27T01:49:30.475073Z","iopub.status.idle":"2023-11-27T01:49:31.715427Z","shell.execute_reply.started":"2023-11-27T01:49:30.475038Z","shell.execute_reply":"2023-11-27T01:49:31.714299Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(type(x))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:31.716753Z","iopub.execute_input":"2023-11-27T01:49:31.717100Z","iopub.status.idle":"2023-11-27T01:49:31.722000Z","shell.execute_reply.started":"2023-11-27T01:49:31.717074Z","shell.execute_reply":"2023-11-27T01:49:31.721076Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"x=torch.tensor(x)\ny=torch.Tensor(y_one_hot)\n\ndataset=TensorDataset(x,y)\n\ntrain_size=int(0.6*len(x))\nval_size=int(0.2*len(x))\ntemp_size=2*val_size\n\n#print(train_size,val_size,temp_size)\n\ntrain_dataset, temp_dataset = random_split(dataset, [train_size, temp_size])\nval_dataset,test_dataset=random_split(temp_dataset, [val_size,val_size])\n\n# Define batch size\n#batch_size = 8\n\n# Create a DataLoader for shuffling and batching\n'''train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\ntest_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:31.723300Z","iopub.execute_input":"2023-11-27T01:49:31.723652Z","iopub.status.idle":"2023-11-27T01:49:33.079474Z","shell.execute_reply.started":"2023-11-27T01:49:31.723617Z","shell.execute_reply":"2023-11-27T01:49:33.078622Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2762140771.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  y=torch.Tensor(y_one_hot)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\\ntest_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)'"},"metadata":{}}]},{"cell_type":"markdown","source":"**DEFINING FUNCTIONS FOR THE FINE-TUNING TASK**","metadata":{}},{"cell_type":"code","source":"def train_and_val(backbone):\n    \n    #training the models first on train_dataloader\n    \n    models=[]\n    for i in range(2):\n        models.append(ResnetSingleChannel(backbone,4))\n        \n   \n    \n    batch_sizes=[64,32]\n    \n    for model_no in range(2):\n        print(f'TRAINING MODEL {model_no+1}')\n        print('--------------------------------------------------------------')\n        fine_tune_opt = optim.Adam(models[model_no].parameters(), lr=0.001, weight_decay=0.0001)\n        ft_loss_fn= nn.CrossEntropyLoss()\n        \n        batch_size=batch_sizes[model_no]\n        \n        train_dataloader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n        \n\n        num_epochs = 8\n        total_steps=train_size//batch_size\n\n        for epoch in range(num_epochs):\n            i=0\n\n            for x_batch,y_batch in train_dataloader:\n                i+=1\n                outputs=models[model_no](torch.Tensor(x_batch))\n                \n                '''print('out',outputs.shape)\n                print('y',y_batch.shape)\n                print('out',outputs)\n                print('y',y_batch)'''\n                \n                loss = ft_loss_fn(outputs, y_batch)\n                fine_tune_opt.zero_grad()\n                loss.backward()\n                fine_tune_opt.step()\n\n                if (i+1) % 20 == 0:\n                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n    \n    # performing validation on validation data_loader\n    best_model=-1\n    best_loss=np.inf\n    for model_no in range(2):\n        \n        loss_sum=0\n        \n        for x_batch,y_batch in val_dataloader:\n            outputs=models[model_no](torch.Tensor(x_batch))\n            loss = ft_loss_fn(outputs, y_batch)\n            loss_sum+=loss.item()\n            \n        if(loss_sum<best_loss):\n            best_model=model_no\n            best_loss=loss_sum\n    \n    return models[best_model]\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:33.080691Z","iopub.execute_input":"2023-11-27T01:49:33.081006Z","iopub.status.idle":"2023-11-27T01:49:33.092887Z","shell.execute_reply.started":"2023-11-27T01:49:33.080980Z","shell.execute_reply":"2023-11-27T01:49:33.091912Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    return F.softmax(x, dim=-1)\n\ndef metrics(model):\n    \n    test_dataloader=DataLoader(test_dataset,batch_size=1,shuffle=True)\n    \n    y_pred_M=[]\n    y_true_M=[]\n    c=0\n    logloss_sum=0\n    \n    for x_batch,y_batch in test_dataloader:\n        \n        c+=1\n        y_pred=model(x_batch)\n\n        '''y_pred_M.append(np.argmax(y_pred.detach().numpy()))\n        y_true_M.append(np.argmax(y_batch.detach().numpy()))'''\n\n        softmax_probs = softmax(y_pred)\n\n        '''print(y_pred)\n        print(softmax_probs)\n        print(y_batch)'''\n\n        logloss = log_loss(y_batch.detach().numpy(), softmax_probs.detach().numpy())\n\n        logloss_sum+=logloss\n        #print(logloss) \n\n    return(logloss_sum/c)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:33.094114Z","iopub.execute_input":"2023-11-27T01:49:33.094393Z","iopub.status.idle":"2023-11-27T01:49:33.107265Z","shell.execute_reply.started":"2023-11-27T01:49:33.094369Z","shell.execute_reply":"2023-11-27T01:49:33.106380Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#MAKING A NEW NETWORK WHICH USES THE BACKBONE OF THE PRETRAINED MODEL, AND HAS AN INPUT LAYER TO TAKE IN A SINGLE CHANNEL IMAGE, AND THEN PASS INTO \n#BACKBONE, AND FINALLY OUTPUT LAYER WHICH PERFORMS REGRESSION\n\nclass ResnetSingleChannel(nn.Module):\n    def __init__(self,backbone,channels):\n        super(ResnetSingleChannel,self).__init__()\n        self.input=nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1, bias=False)\n        self.backbone=backbone\n        self.output=nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.Linear(in_features=256, out_features=channels)\n        )\n        \n    def forward(self,x):\n        x=self.input(x)\n        x=self.backbone(x)\n        x=self.output(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:33.108243Z","iopub.execute_input":"2023-11-27T01:49:33.108491Z","iopub.status.idle":"2023-11-27T01:49:33.118371Z","shell.execute_reply.started":"2023-11-27T01:49:33.108468Z","shell.execute_reply":"2023-11-27T01:49:33.117616Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**TRAINING THE SIMCLR MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in SimCLR_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSC_MODEL=train_and_val(SimCLR_ImagenetPT_model.backbone)\nSC_M_2=metrics(SC_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T01:49:33.119340Z","iopub.execute_input":"2023-11-27T01:49:33.119598Z","iopub.status.idle":"2023-11-27T03:16:31.227660Z","shell.execute_reply.started":"2023-11-27T01:49:33.119575Z","shell.execute_reply":"2023-11-27T03:16:31.226492Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 0.8027\nEpoch [1/8], Step [40/60], Loss: 0.7738\nEpoch [1/8], Step [60/60], Loss: 0.8522\nEpoch [2/8], Step [20/60], Loss: 0.8036\nEpoch [2/8], Step [40/60], Loss: 0.7595\nEpoch [2/8], Step [60/60], Loss: 0.8265\nEpoch [3/8], Step [20/60], Loss: 0.6659\nEpoch [3/8], Step [40/60], Loss: 0.7739\nEpoch [3/8], Step [60/60], Loss: 0.7084\nEpoch [4/8], Step [20/60], Loss: 0.5890\nEpoch [4/8], Step [40/60], Loss: 0.7505\nEpoch [4/8], Step [60/60], Loss: 0.5218\nEpoch [5/8], Step [20/60], Loss: 0.2775\nEpoch [5/8], Step [40/60], Loss: 0.3202\nEpoch [5/8], Step [60/60], Loss: 0.2941\nEpoch [6/8], Step [20/60], Loss: 0.2345\nEpoch [6/8], Step [40/60], Loss: 0.2341\nEpoch [6/8], Step [60/60], Loss: 0.4450\nEpoch [7/8], Step [20/60], Loss: 0.2509\nEpoch [7/8], Step [40/60], Loss: 0.2084\nEpoch [7/8], Step [60/60], Loss: 0.1248\nEpoch [8/8], Step [20/60], Loss: 0.1123\nEpoch [8/8], Step [40/60], Loss: 0.1447\nEpoch [8/8], Step [60/60], Loss: 0.1600\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 0.9373\nEpoch [1/8], Step [40/120], Loss: 0.9958\nEpoch [1/8], Step [60/120], Loss: 0.8974\nEpoch [1/8], Step [80/120], Loss: 0.7410\nEpoch [1/8], Step [100/120], Loss: 0.8007\nEpoch [1/8], Step [120/120], Loss: 0.9596\nEpoch [2/8], Step [20/120], Loss: 0.6275\nEpoch [2/8], Step [40/120], Loss: 0.7902\nEpoch [2/8], Step [60/120], Loss: 0.5385\nEpoch [2/8], Step [80/120], Loss: 0.6623\nEpoch [2/8], Step [100/120], Loss: 0.7943\nEpoch [2/8], Step [120/120], Loss: 0.7564\nEpoch [3/8], Step [20/120], Loss: 0.4292\nEpoch [3/8], Step [40/120], Loss: 1.0010\nEpoch [3/8], Step [60/120], Loss: 0.4989\nEpoch [3/8], Step [80/120], Loss: 0.5543\nEpoch [3/8], Step [100/120], Loss: 0.6029\nEpoch [3/8], Step [120/120], Loss: 0.3819\nEpoch [4/8], Step [20/120], Loss: 0.3705\nEpoch [4/8], Step [40/120], Loss: 0.3518\nEpoch [4/8], Step [60/120], Loss: 0.1083\nEpoch [4/8], Step [80/120], Loss: 0.2157\nEpoch [4/8], Step [100/120], Loss: 0.1248\nEpoch [4/8], Step [120/120], Loss: 0.2181\nEpoch [5/8], Step [20/120], Loss: 0.3938\nEpoch [5/8], Step [40/120], Loss: 0.2628\nEpoch [5/8], Step [60/120], Loss: 0.5032\nEpoch [5/8], Step [80/120], Loss: 0.1245\nEpoch [5/8], Step [100/120], Loss: 0.1454\nEpoch [5/8], Step [120/120], Loss: 0.1700\nEpoch [6/8], Step [20/120], Loss: 0.2577\nEpoch [6/8], Step [40/120], Loss: 0.0839\nEpoch [6/8], Step [60/120], Loss: 0.1701\nEpoch [6/8], Step [80/120], Loss: 0.1479\nEpoch [6/8], Step [100/120], Loss: 0.2487\nEpoch [6/8], Step [120/120], Loss: 0.0866\nEpoch [7/8], Step [20/120], Loss: 0.1299\nEpoch [7/8], Step [40/120], Loss: 0.0828\nEpoch [7/8], Step [60/120], Loss: 0.0261\nEpoch [7/8], Step [80/120], Loss: 0.2134\nEpoch [7/8], Step [100/120], Loss: 0.1277\nEpoch [7/8], Step [120/120], Loss: 0.0366\nEpoch [8/8], Step [20/120], Loss: 0.0519\nEpoch [8/8], Step [40/120], Loss: 0.0061\nEpoch [8/8], Step [60/120], Loss: 0.0330\nEpoch [8/8], Step [80/120], Loss: 0.0979\nEpoch [8/8], Step [100/120], Loss: 0.0774\nEpoch [8/8], Step [120/120], Loss: 0.0126\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TRAINING THE BARLOW TWINS MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in BT_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nBT_MODEL=train_and_val(BT_ImagenetPT_model.backbone)\nBT_M_2=metrics(BT_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:16:31.229415Z","iopub.execute_input":"2023-11-27T03:16:31.230166Z","iopub.status.idle":"2023-11-27T05:01:26.864042Z","shell.execute_reply.started":"2023-11-27T03:16:31.230126Z","shell.execute_reply":"2023-11-27T05:01:26.862750Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 2.1428\nEpoch [1/8], Step [40/60], Loss: 1.5571\nEpoch [1/8], Step [60/60], Loss: 1.7827\nEpoch [2/8], Step [20/60], Loss: 1.1032\nEpoch [2/8], Step [40/60], Loss: 1.1240\nEpoch [2/8], Step [60/60], Loss: 1.0547\nEpoch [3/8], Step [20/60], Loss: 1.1180\nEpoch [3/8], Step [40/60], Loss: 1.1068\nEpoch [3/8], Step [60/60], Loss: 1.0433\nEpoch [4/8], Step [20/60], Loss: 0.9108\nEpoch [4/8], Step [40/60], Loss: 1.1899\nEpoch [4/8], Step [60/60], Loss: 1.2018\nEpoch [5/8], Step [20/60], Loss: 1.4657\nEpoch [5/8], Step [40/60], Loss: 0.9067\nEpoch [5/8], Step [60/60], Loss: 0.9788\nEpoch [6/8], Step [20/60], Loss: 1.2346\nEpoch [6/8], Step [40/60], Loss: 1.1534\nEpoch [6/8], Step [60/60], Loss: 1.5096\nEpoch [7/8], Step [20/60], Loss: 1.0944\nEpoch [7/8], Step [40/60], Loss: 1.0947\nEpoch [7/8], Step [60/60], Loss: 1.0042\nEpoch [8/8], Step [20/60], Loss: 1.1918\nEpoch [8/8], Step [40/60], Loss: 0.9954\nEpoch [8/8], Step [60/60], Loss: 1.3495\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 4.4166\nEpoch [1/8], Step [40/120], Loss: 3.6197\nEpoch [1/8], Step [60/120], Loss: 1.7291\nEpoch [1/8], Step [80/120], Loss: 1.9937\nEpoch [1/8], Step [100/120], Loss: 1.1215\nEpoch [1/8], Step [120/120], Loss: 1.1994\nEpoch [2/8], Step [20/120], Loss: 1.9530\nEpoch [2/8], Step [40/120], Loss: 1.1099\nEpoch [2/8], Step [60/120], Loss: 1.9171\nEpoch [2/8], Step [80/120], Loss: 1.4811\nEpoch [2/8], Step [100/120], Loss: 1.3321\nEpoch [2/8], Step [120/120], Loss: 1.3815\nEpoch [3/8], Step [20/120], Loss: 0.8964\nEpoch [3/8], Step [40/120], Loss: 1.1651\nEpoch [3/8], Step [60/120], Loss: 1.0109\nEpoch [3/8], Step [80/120], Loss: 1.1623\nEpoch [3/8], Step [100/120], Loss: 1.9828\nEpoch [3/8], Step [120/120], Loss: 0.9645\nEpoch [4/8], Step [20/120], Loss: 1.4135\nEpoch [4/8], Step [40/120], Loss: 0.8941\nEpoch [4/8], Step [60/120], Loss: 1.1087\nEpoch [4/8], Step [80/120], Loss: 1.2333\nEpoch [4/8], Step [100/120], Loss: 1.4087\nEpoch [4/8], Step [120/120], Loss: 1.5829\nEpoch [5/8], Step [20/120], Loss: 0.8797\nEpoch [5/8], Step [40/120], Loss: 0.9948\nEpoch [5/8], Step [60/120], Loss: 0.9951\nEpoch [5/8], Step [80/120], Loss: 0.9030\nEpoch [5/8], Step [100/120], Loss: 1.1210\nEpoch [5/8], Step [120/120], Loss: 1.2997\nEpoch [6/8], Step [20/120], Loss: 0.8347\nEpoch [6/8], Step [40/120], Loss: 1.1287\nEpoch [6/8], Step [60/120], Loss: 1.0637\nEpoch [6/8], Step [80/120], Loss: 0.9884\nEpoch [6/8], Step [100/120], Loss: 0.9633\nEpoch [6/8], Step [120/120], Loss: 0.9428\nEpoch [7/8], Step [20/120], Loss: 0.9204\nEpoch [7/8], Step [40/120], Loss: 0.9684\nEpoch [7/8], Step [60/120], Loss: 1.0459\nEpoch [7/8], Step [80/120], Loss: 0.8965\nEpoch [7/8], Step [100/120], Loss: 1.0548\nEpoch [7/8], Step [120/120], Loss: 1.2876\nEpoch [8/8], Step [20/120], Loss: 1.0035\nEpoch [8/8], Step [40/120], Loss: 1.1462\nEpoch [8/8], Step [60/120], Loss: 1.4376\nEpoch [8/8], Step [80/120], Loss: 0.9451\nEpoch [8/8], Step [100/120], Loss: 0.9563\nEpoch [8/8], Step [120/120], Loss: 0.6959\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TRAINING THE SWAV MODEL**","metadata":{}},{"cell_type":"code","source":"for layer in SW_ImagenetPT_model.backbone:\n    for param in layer.parameters():\n        param.requires_grad = True\n        \nSW_MODEL=train_and_val(SW_ImagenetPT_model.backbone)\nSW_M_2=metrics(SW_MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:01:26.865657Z","iopub.execute_input":"2023-11-27T05:01:26.866027Z","iopub.status.idle":"2023-11-27T06:29:07.694375Z","shell.execute_reply.started":"2023-11-27T05:01:26.865997Z","shell.execute_reply":"2023-11-27T06:29:07.693265Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"TRAINING MODEL 1\n--------------------------------------------------------------\nEpoch [1/8], Step [20/60], Loss: 0.9670\nEpoch [1/8], Step [40/60], Loss: 0.9265\nEpoch [1/8], Step [60/60], Loss: 0.9607\nEpoch [2/8], Step [20/60], Loss: 0.9149\nEpoch [2/8], Step [40/60], Loss: 0.7802\nEpoch [2/8], Step [60/60], Loss: 0.7936\nEpoch [3/8], Step [20/60], Loss: 0.9372\nEpoch [3/8], Step [40/60], Loss: 0.8128\nEpoch [3/8], Step [60/60], Loss: 0.8470\nEpoch [4/8], Step [20/60], Loss: 0.7797\nEpoch [4/8], Step [40/60], Loss: 0.8589\nEpoch [4/8], Step [60/60], Loss: 0.7988\nEpoch [5/8], Step [20/60], Loss: 0.8519\nEpoch [5/8], Step [40/60], Loss: 0.8527\nEpoch [5/8], Step [60/60], Loss: 0.8202\nEpoch [6/8], Step [20/60], Loss: 0.7973\nEpoch [6/8], Step [40/60], Loss: 0.6777\nEpoch [6/8], Step [60/60], Loss: 0.6443\nEpoch [7/8], Step [20/60], Loss: 0.7141\nEpoch [7/8], Step [40/60], Loss: 0.6566\nEpoch [7/8], Step [60/60], Loss: 0.7454\nEpoch [8/8], Step [20/60], Loss: 0.4946\nEpoch [8/8], Step [40/60], Loss: 0.5801\nEpoch [8/8], Step [60/60], Loss: 0.6279\nTRAINING MODEL 2\n--------------------------------------------------------------\nEpoch [1/8], Step [20/120], Loss: 0.9091\nEpoch [1/8], Step [40/120], Loss: 0.9911\nEpoch [1/8], Step [60/120], Loss: 0.8602\nEpoch [1/8], Step [80/120], Loss: 0.8705\nEpoch [1/8], Step [100/120], Loss: 0.9379\nEpoch [1/8], Step [120/120], Loss: 0.8505\nEpoch [2/8], Step [20/120], Loss: 0.6785\nEpoch [2/8], Step [40/120], Loss: 0.8687\nEpoch [2/8], Step [60/120], Loss: 0.9495\nEpoch [2/8], Step [80/120], Loss: 0.8161\nEpoch [2/8], Step [100/120], Loss: 1.0125\nEpoch [2/8], Step [120/120], Loss: 0.7836\nEpoch [3/8], Step [20/120], Loss: 0.9457\nEpoch [3/8], Step [40/120], Loss: 0.8811\nEpoch [3/8], Step [60/120], Loss: 0.6778\nEpoch [3/8], Step [80/120], Loss: 0.8114\nEpoch [3/8], Step [100/120], Loss: 0.6267\nEpoch [3/8], Step [120/120], Loss: 0.7843\nEpoch [4/8], Step [20/120], Loss: 0.6480\nEpoch [4/8], Step [40/120], Loss: 0.8636\nEpoch [4/8], Step [60/120], Loss: 0.7674\nEpoch [4/8], Step [80/120], Loss: 0.8215\nEpoch [4/8], Step [100/120], Loss: 0.5658\nEpoch [4/8], Step [120/120], Loss: 0.6316\nEpoch [5/8], Step [20/120], Loss: 0.5867\nEpoch [5/8], Step [40/120], Loss: 0.7648\nEpoch [5/8], Step [60/120], Loss: 0.5732\nEpoch [5/8], Step [80/120], Loss: 1.0802\nEpoch [5/8], Step [100/120], Loss: 0.7526\nEpoch [5/8], Step [120/120], Loss: 0.5389\nEpoch [6/8], Step [20/120], Loss: 0.5412\nEpoch [6/8], Step [40/120], Loss: 0.3665\nEpoch [6/8], Step [60/120], Loss: 0.5244\nEpoch [6/8], Step [80/120], Loss: 0.5122\nEpoch [6/8], Step [100/120], Loss: 0.7065\nEpoch [6/8], Step [120/120], Loss: 0.6566\nEpoch [7/8], Step [20/120], Loss: 0.4106\nEpoch [7/8], Step [40/120], Loss: 0.1839\nEpoch [7/8], Step [60/120], Loss: 0.6114\nEpoch [7/8], Step [80/120], Loss: 0.6048\nEpoch [7/8], Step [100/120], Loss: 0.4395\nEpoch [7/8], Step [120/120], Loss: 0.6875\nEpoch [8/8], Step [20/120], Loss: 0.4562\nEpoch [8/8], Step [40/120], Loss: 0.1331\nEpoch [8/8], Step [60/120], Loss: 0.3282\nEpoch [8/8], Step [80/120], Loss: 0.4025\nEpoch [8/8], Step [100/120], Loss: 0.1215\nEpoch [8/8], Step [120/120], Loss: 0.3790\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PRINTING THE PERFORMANCE METRICS**","metadata":{}},{"cell_type":"code","source":"print('SimCLR',SC_M_2)\nprint('Barlow',BT_M_2)\nprint('Swav',SW_M_2)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:41:22.866557Z","iopub.execute_input":"2023-11-27T06:41:22.867153Z","iopub.status.idle":"2023-11-27T06:41:22.872679Z","shell.execute_reply.started":"2023-11-27T06:41:22.867121Z","shell.execute_reply":"2023-11-27T06:41:22.871648Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"SimCLR 1.5859438103475703\nBarlow 1.10219588117122\nSwav 1.1838900749619081\n","output_type":"stream"}]}]}